{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e9b6ff-2a8a-4a8d-ba16-4a5bdcf2d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: C:\\Users\\HP\\Desktop\\projects 2025\\Binary Classification with a Bank Dataset\\data\n",
      "{'train': (750000, 18), 'test': (250000, 17)}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path(r\"C:\\Users\\HP\\Desktop\\projects 2025\\Binary Classification with a Bank Dataset\")\n",
    "data_dir = project_root / \"data\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"train_processed.csv\")\n",
    "test = pd.read_csv(data_dir / \"test_processed.csv\")\n",
    "sample = pd.read_csv(data_dir / \"sample_submission.csv\")\n",
    "\n",
    "print(\"Using:\", data_dir)\n",
    "print({\"train\": train.shape, \"test\": test.shape})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62e1a2f-1549-4a6d-b0c6-f20490ba8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   id         750000 non-null  float64\n",
      " 1   age        750000 non-null  float64\n",
      " 2   job        750000 non-null  float64\n",
      " 3   marital    750000 non-null  float64\n",
      " 4   education  750000 non-null  float64\n",
      " 5   default    750000 non-null  float64\n",
      " 6   balance    750000 non-null  float64\n",
      " 7   housing    750000 non-null  float64\n",
      " 8   loan       750000 non-null  float64\n",
      " 9   contact    750000 non-null  float64\n",
      " 10  day        750000 non-null  float64\n",
      " 11  month      750000 non-null  float64\n",
      " 12  duration   750000 non-null  float64\n",
      " 13  campaign   750000 non-null  float64\n",
      " 14  pdays      750000 non-null  float64\n",
      " 15  previous   750000 non-null  float64\n",
      " 16  poutcome   750000 non-null  float64\n",
      " 17  y          750000 non-null  int64  \n",
      "dtypes: float64(17), int64(1)\n",
      "memory usage: 103.0 MB\n",
      "None\n",
      "Top missing:\n",
      " id           0\n",
      "age          0\n",
      "poutcome     0\n",
      "previous     0\n",
      "pdays        0\n",
      "campaign     0\n",
      "duration     0\n",
      "month        0\n",
      "day          0\n",
      "contact      0\n",
      "loan         0\n",
      "housing      0\n",
      "balance      0\n",
      "default      0\n",
      "education    0\n",
      "dtype: int64\n",
      "Target balance: {0: 0.8793, 1: 0.1207}\n"
     ]
    }
   ],
   "source": [
    "print(train.info())\n",
    "missing = train.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Top missing:\\n\", missing.head(15))\n",
    "if 'y' in train.columns:\n",
    "    print(\"Target balance:\", train['y'].value_counts(normalize=True).round(4).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec17e103-ee13-45e1-9d97-35424cf08d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: 18 Categorical: 0\n",
      "Only in train: ['y']\n",
      "Only in test: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Numeric:\", len(num_cols), \"Categorical:\", len(cat_cols))\n",
    "print(\"Only in train:\", sorted(set(train.columns)-set(test.columns)))\n",
    "print(\"Only in test:\", sorted(set(test.columns)-set(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3550f75-94d8-4939-b839-88ab606877f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most skewed (top 10):\n",
      " default      7.442308\n",
      "loan         2.075377\n",
      "duration     1.223740\n",
      "campaign     1.188024\n",
      "balance      0.945033\n",
      "contact      0.722495\n",
      "age          0.498202\n",
      "job          0.273263\n",
      "education    0.131906\n",
      "day          0.054014\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Boolean array expected for the condition, not float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13176\\4153136460.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m skew = train[num_cols].drop(columns=[\u001b[33m'y'\u001b[39m], errors=\u001b[33m'ignore'\u001b[39m).skew(numeric_only=\u001b[38;5;28;01mTrue\u001b[39;00m).sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      2\u001b[39m print(\u001b[33m\"Most skewed (top 10):\\n\"\u001b[39m, skew.head(\u001b[32m10\u001b[39m))\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m corr = train[num_cols].corr(numeric_only=\u001b[38;5;28;01mTrue\u001b[39;00m).abs()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m pairs = corr.where(np.triu(np.ones(corr.shape), \u001b[32m1\u001b[39m)).stack().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      6\u001b[39m print(\u001b[33m\"Highly correlated pairs (>0.9):\\n\"\u001b[39m, pairs[pairs > \u001b[32m0.9\u001b[39m].head(\u001b[32m15\u001b[39m))\n",
      "\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, cond, other, inplace, axis, level)\u001b[39m\n\u001b[32m  10999\u001b[39m                         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m  11000\u001b[39m                     )\n\u001b[32m  11001\u001b[39m \n\u001b[32m  11002\u001b[39m         other = common.apply_if_callable(other, self)\n\u001b[32m> \u001b[39m\u001b[32m11003\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._where(cond, other, inplace, axis, level)\n",
      "\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, cond, other, inplace, axis, level, warn)\u001b[39m\n\u001b[32m  10688\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m ValueError(msg.format(dtype=cond.dtype))\n\u001b[32m  10689\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m  10690\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m _dt \u001b[38;5;28;01min\u001b[39;00m cond.dtypes:\n\u001b[32m  10691\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_bool_dtype(_dt):\n\u001b[32m> \u001b[39m\u001b[32m10692\u001b[39m                         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(msg.format(dtype=_dt))\n\u001b[32m  10693\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m cond._mgr.any_extension_types:\n\u001b[32m  10694\u001b[39m                     \u001b[38;5;66;03m# GH51574: avoid object ndarray conversion later on\u001b[39;00m\n\u001b[32m  10695\u001b[39m                     cond = cond._constructor(\n",
      "\u001b[31mValueError\u001b[39m: Boolean array expected for the condition, not float64"
     ]
    }
   ],
   "source": [
    "skew = train[num_cols].drop(columns=['y'], errors='ignore').skew(numeric_only=True).sort_values(ascending=False)\n",
    "print(\"Most skewed (top 10):\\n\", skew.head(10))\n",
    "\n",
    "corr = train[num_cols].corr(numeric_only=True).abs()\n",
    "pairs = corr.where(np.triu(np.ones(corr.shape), 1)).stack().sort_values(ascending=False)\n",
    "print(\"Highly correlated pairs (>0.9):\\n\", pairs[pairs > 0.9].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75eedff-1395-42df-aacd-ac56cc3a3567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated pairs (>0.9):\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "corr = train[num_cols].corr(numeric_only=True).abs()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "pairs = corr.where(mask).stack().sort_values(ascending=False)\n",
    "print(\"Highly correlated pairs (>0.9):\")\n",
    "print(pairs[pairs > 0.9].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008b6268-5e78-44a9-9064-d4c78b5d6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "corr = train[num_cols].corr(numeric_only=True).abs()\n",
    "ix = np.triu_indices_from(corr, k=1)\n",
    "pairs = pd.Series(corr.values[ix], index=[(corr.index[i], corr.columns[j]) for i, j in zip(*ix)])\n",
    "print(pairs[pairs > 0.9].sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9c0ce7-4392-4232-8d2d-8a256a69e9ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_processed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the data you trained on (same source you passed to cv_train)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/train_processed.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or read from combined_fe then filter split=='train'\u001b[39;00m\n\u001b[32m      6\u001b[39m X = train.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m y = train[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train_processed.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the data you trained on (same source you passed to cv_train)\n",
    "train = pd.read_csv(\"data/train_processed.csv\")  # or read from combined_fe then filter split=='train'\n",
    "X = train.drop(columns=[\"y\"])\n",
    "y = train[\"y\"].astype(int)\n",
    "\n",
    "# 1) Check if any feature equals y or is perfectly correlated\n",
    "perfect_cols = []\n",
    "for c in X.columns:\n",
    "    if X[c].equals(y) or X[c].equals(1 - y):\n",
    "        perfect_cols.append(c)\n",
    "print(\"Features identical/inverted to y:\", perfect_cols)\n",
    "\n",
    "# 2) Check near-perfect single-feature AUCs\n",
    "suspicious = []\n",
    "for c in X.columns:\n",
    "    try:\n",
    "        auc = roc_auc_score(y, pd.Series(X[c]).fillna(pd.Series(X[c]).median()))\n",
    "        if auc >= 0.999 or auc <= 0.001:\n",
    "            suspicious.append((c, float(auc)))\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"Suspicious (AUC ~ 1):\", suspicious)\n",
    "\n",
    "# 3) If using combined_fe, ensure 'split' isn't in features and that 'y' is not leaked\n",
    "print(\"Columns that look like split/y flags present in X:\",\n",
    "      [c for c in X.columns if c.lower() in {\"split\",\"y\",\"target\",\"label\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f81c95-2cdd-4cb8-99f5-049a74c46cf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\combined_fe.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Point to the dataset you passed via --use-combined\u001b[39;00m\n\u001b[32m      5\u001b[39m path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcombined_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# or r\"data\\combined.parquet\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m pd.read_csv(path)\n\u001b[32m      8\u001b[39m train = df[df[\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].drop(columns=[\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m      9\u001b[39m X = train.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\combined_fe.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Point to the dataset you passed via --use-combined\n",
    "path = r\"data\\combined_fe.parquet\"  # or r\"data\\combined.parquet\"\n",
    "df = pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "\n",
    "train = df[df[\"split\"]==\"train\"].drop(columns=[\"split\"]).copy()\n",
    "X = train.drop(columns=[\"y\"])\n",
    "y = train[\"y\"].astype(int)\n",
    "\n",
    "# 1) Exact matches to y (or inverted)\n",
    "perfect = [c for c in X.columns if X[c].equals(y) or X[c].equals(1 - y)]\n",
    "print(\"Features identical/inverted to y:\", perfect)\n",
    "\n",
    "# 2) Near-perfect single-feature AUCs\n",
    "suspicious = []\n",
    "for c in X.columns:\n",
    "    try:\n",
    "        v = X[c]\n",
    "        auc = roc_auc_score(y, v.fillna(v.median()))\n",
    "        if auc >= 0.999 or auc <= 0.001:\n",
    "            suspicious.append((c, float(auc)))\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"Suspicious (AUC ~ 1):\", suspicious)\n",
    "\n",
    "# 3) Columns that look like flags\n",
    "print(\"Flag-like columns present:\",\n",
    "      [c for c in X.columns if c.lower() in {\"split\",\"y\",\"target\",\"label\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee624e1-c916-4029-8858-717fe4e7e022",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1561518968.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcd \"C:\\Users\\HP\\Desktop\\projects 2025\\Binary Classification with a Bank Dataset\"\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\HP\\Desktop\\projects 2025\\Binary Classification with a Bank Dataset\"\n",
    "git status\n",
    "\n",
    "# Stage modified + new files\n",
    "git add notebooks/Untitled.ipynb scripts/cv_train.py scripts/data_preprocessing.py scripts/data_integration.py scripts/feature_engineering.py\n",
    "\n",
    "# Commit with a concise message\n",
    "git commit -m \"feat: add data integration & feature engineering; update CV training and preprocessing; tweak EDA notebook\"\n",
    "\n",
    "# Push to origin/main\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a207a-c8b7-4041-8b0b-5e33aab0a824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
